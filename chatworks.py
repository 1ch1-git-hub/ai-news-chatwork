#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
chatworks.py

æ¯æœ9æ™‚ã«æ—¥æœ¬èªã®"AI"é–¢é€£è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼‹ãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã€
æŒ‡å®šã®ChatWorkãƒ«ãƒ¼ãƒ ã¸æŠ•ç¨¿ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

äº‹å‰æº–å‚™:
    pip install requests beautifulsoup4[lxml]
"""

import sys
import logging
import requests
import os
from datetime import datetime
from bs4 import BeautifulSoup

# â€”â€”â€” ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾— â€”â€”â€”
CHATWORK_TOKEN   = os.environ.get("CHATWORK_TOKEN")
CHATWORK_ROOM_ID = os.environ.get("CHATWORK_ROOM_ID")
NEWS_LIMIT       = 8

# è¨­å®šãƒã‚§ãƒƒã‚¯
if not CHATWORK_TOKEN or not CHATWORK_ROOM_ID:
    logging.error("ç’°å¢ƒå¤‰æ•° CHATWORK_TOKEN ã¨ CHATWORK_ROOM_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# æ—¥æœ¬èªã‚µã‚¤ãƒˆã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ã®URLã«ä¿®æ­£ï¼‰
FEED_URLS = [
    # ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ
    "https://news.google.com/rss/search?q=AI&hl=ja&gl=JP&ceid=JP:ja",
    "https://news.yahoo.co.jp/rss/topics/it.xml",
    "https://feeds.reuters.com/reuters/JPtechnologyNews",
    
    # ITç³»å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
    "https://rss.itmedia.co.jp/rss/2.0/itmedia_news.xml",
    "https://rss.itmedia.co.jp/rss/2.0/itmedia_aiplus.xml",
    "http://feed.japan.cnet.com/rss/index.rdf",
    "https://ascii.jp/rss.xml",
    "https://www.watch.impress.co.jp/data/rss/1.0/ipw/feed.rdf",
    "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
    "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf",
    "https://akiba-pc.watch.impress.co.jp/data/rss/1.0/ah/feed.rdf",
    "https://forest.watch.impress.co.jp/data/rss/1.0/wf/feed.rdf",
    "https://gigazine.net/news/rss_2.0/",
    
    # æŠ€è¡“ç³»ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
    "https://www.publickey1.jp/atom.xml",
    "https://codezine.jp/rss/new/20/index.xml",
    "https://gihyo.jp/feed/rss2",
    "https://xtech.nikkei.com/rss/index.rdf",
    "https://weekly.ascii.jp/rss.xml",
    
    # ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆç³»ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
    "https://toyokeizai.net/list/feed/rss",
    "https://diamond.jp/list/feed/rss",
    
    # ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ»ITç³»
    "https://thebridge.jp/feed",
    "https://www.sbbit.jp/rss/HotTopics.rss",
    
    # AIç‰¹åŒ–ãƒ¡ãƒ‡ã‚£ã‚¢
    "https://ainow.ai/feed/",
    
    # ä¼æ¥­ãƒ–ãƒ­ã‚°ãƒ»ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚°ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
    "https://developers.cyberagent.co.jp/blog/feed/",
    "https://techblog.yahoo.co.jp/atom.xml",
    "https://developer.hatenastaff.com/rss",
    "https://blog.recruit.co.jp/rtc/feed/",
    
    # è¿½åŠ ã®æŠ€è¡“ç³»ã‚µã‚¤ãƒˆ
    "https://qiita.com/popular-items/feed",
    "https://zenn.dev/feed",
    
    # å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢
    "https://resou.osaka-u.ac.jp/ja/rss.xml",
    
    # ãã®ä»–ã®ITç³»ãƒ¡ãƒ‡ã‚£ã‚¢
    "https://it.srad.jp/srad.rdf",
    "https://japan.zdnet.com/rss/index.rdf",
]

REQUEST_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/102.0.0.0 Safari/537.36"
    )
}

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)


def get_source_name(url: str) -> str:
    """
    URLã‹ã‚‰æƒ…å ±æºã®åå‰ã‚’å–å¾—
    """
    source_mapping = {
        "news.google.com": "Google ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "news.yahoo.co.jp": "Yahoo! ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "feeds.reuters.com": "Reuters Japan",
        "itmedia.co.jp": "ITmedia",
        "feed.japan.cnet.com": "CNET Japan",
        "ascii.jp": "ASCII.jp",
        "watch.impress.co.jp": "Impress Watch",
        "internet.watch.impress.co.jp": "INTERNET Watch", 
        "pc.watch.impress.co.jp": "PC Watch",
        "akiba-pc.watch.impress.co.jp": "AKIBA PC Hotline!",
        "forest.watch.impress.co.jp": "çª“ã®æœ",
        "gigazine.net": "GIGAZINE",
        "publickey1.jp": "Publickey",
        "codezine.jp": "CodeZine",
        "gihyo.jp": "æŠ€è¡“è©•è«–ç¤¾",
        "xtech.nikkei.com": "æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒ†ãƒƒã‚¯",
        "weekly.ascii.jp": "é€±åˆŠã‚¢ã‚¹ã‚­ãƒ¼",
        "toyokeizai.net": "æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
        "diamond.jp": "ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
        "thebridge.jp": "THE BRIDGE",
        "sbbit.jp": "SB Creative",
        "ainow.ai": "AINOW",
        "developers.cyberagent.co.jp": "CyberAgent",
        "techblog.yahoo.co.jp": "Yahoo! JAPAN Tech Blog",
        "developer.hatenastaff.com": "Hatena Developer Blog",
        "blog.recruit.co.jp": "Recruit Tech Blog",
        "qiita.com": "Qiita",
        "zenn.dev": "Zenn",
        "osaka-u.ac.jp": "å¤§é˜ªå¤§å­¦",
        "srad.jp": "ã‚¹ãƒ©ãƒ‰",
        "zdnet.com": "ZDNet Japan",
    }
    
    for domain, name in source_mapping.items():
        if domain in url:
            return name
    
    return "ãã®ä»–"


def fetch_ai_news(limit: int = NEWS_LIMIT) -> list[dict]:
    """
    æ—¥æœ¬èªãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰"AI"é–¢é€£è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‹ãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã€
    limit ä»¶ã ã‘è¿”ã™ã€‚
    """
    articles = []
    successful_feeds = 0
    failed_feeds = 0
    
    for url in FEED_URLS:
        try:
            logging.info(f"Fetching feed: {url}")
            resp = requests.get(url, headers=REQUEST_HEADERS, timeout=15, verify=True)
            resp.raise_for_status()
            successful_feeds += 1
        except Exception as e:
            logging.warning(f"ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•— ({get_source_name(url)}): {e}")
            failed_feeds += 1
            continue

        try:
            # é€šå¸¸ã®RSS/Atomãƒ•ã‚£ãƒ¼ãƒ‰ã‚’XMLã§å‡¦ç†
            soup = BeautifulSoup(resp.content, "xml")
            source_name = get_source_name(url)
            
            # RSS ã® item
            for item in soup.find_all("item"):
                title_elem = item.find("title")
                link_elem = item.find("link")
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get_text(strip=True)
                    
                    if title and link:
                        articles.append({
                            "title": title,
                            "link": link,
                            "source": source_name
                        })
            
            # Atom ã® entry
            for entry in soup.find_all("entry"):
                title_elem = entry.find("title")
                link_elem = entry.find("link", rel="alternate") or entry.find("link")
                
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    link = ""
                    
                    if link_elem:
                        if link_elem.has_attr("href"):
                            link = link_elem["href"].strip()
                        else:
                            link = link_elem.get_text(strip=True)
                    
                    if title and link:
                        articles.append({
                            "title": title,
                            "link": link,
                            "source": source_name
                        })
                        
        except Exception as e:
            logging.warning(f"ãƒ•ã‚£ãƒ¼ãƒ‰è§£æå¤±æ•— ({get_source_name(url)}): {e}")

    logging.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—çµæœ: æˆåŠŸ {successful_feeds}ä»¶, å¤±æ•— {failed_feeds}ä»¶")

    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ‹¡å¼µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
    ai_keywords = [
        "ai", "ï¼¡ï¼©", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ãƒã‚·ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", "æ·±å±¤å­¦ç¿’", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°",
        "chatgpt", "ãƒãƒ£ãƒƒãƒˆgpt", "claude", "gemini", "copilot", "gpt", "llm", "ç”Ÿæˆai", "ç”Ÿæˆï¼¡ï¼©",
        "ç”»åƒç”Ÿæˆ", "è‡ªç„¶è¨€èªå‡¦ç†", "nlp", "è‡ªå‹•åŒ–", "ãƒ­ãƒœãƒƒãƒˆ", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "neural",
        "tensorflow", "pytorch", "openai", "google ai", "microsoft ai", "anthropic",
        "è‡ªå‹•é‹è»¢", "éŸ³å£°èªè­˜", "é¡”èªè­˜", "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«", "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹", "ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿"
    ]
    
    filtered = []
    for article in articles:
        title_lower = article["title"].lower()
        if any(keyword in title_lower for keyword in ai_keywords):
            filtered.append(article)

    # é‡è¤‡æ’é™¤ã¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    seen_urls = set()
    seen_titles = set()
    unique = []
    
    # AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
    def calculate_ai_score(title):
        title_lower = title.lower()
        score = 0
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é«˜ã„ã‚¹ã‚³ã‚¢
        high_priority = ["chatgpt", "gpt", "openai", "claude", "gemini", "ç”Ÿæˆai", "ç”Ÿæˆï¼¡ï¼©"]
        medium_priority = ["ai", "ï¼¡ï¼©", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’"]
        
        for keyword in high_priority:
            if keyword in title_lower:
                score += 3
        for keyword in medium_priority:
            if keyword in title_lower:
                score += 1
        return score
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    filtered.sort(key=lambda x: calculate_ai_score(x["title"]), reverse=True)
    
    for article in filtered:
        # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
        if article["link"] in seen_urls:
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼šæœ€åˆã®30æ–‡å­—ã§åˆ¤å®šï¼‰
        title_key = article["title"][:30].strip()
        if title_key in seen_titles:
            continue
        
        seen_urls.add(article["link"])
        seen_titles.add(title_key)
        unique.append(article)
        
        if len(unique) >= limit:
            break

    logging.info(f"Selected {len(unique)} AI articles from {len(filtered)} filtered articles")
    return unique


def post_to_chatwork(message: str) -> None:
    """
    ChatWorkã¸ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æŠ•ç¨¿ã€‚
    """
    url = f"https://api.chatwork.com/v2/rooms/{CHATWORK_ROOM_ID}/messages"
    headers = {
        "X-ChatWorkToken": CHATWORK_TOKEN,
        "Content-Type": "application/x-www-form-urlencoded"
    }
    data = {"body": message}
    resp = requests.post(url, headers=headers, data=data)
    resp.raise_for_status()


def build_news_message(articles: list[dict]) -> str:
    """
    AIãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã®çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã€‚
    """
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M")
    
    # æƒ…å ±æºã®çµ±è¨ˆ
    sources = {}
    for article in articles:
        source = article.get("source", "ä¸æ˜")
        sources[source] = sources.get(source, 0) + 1
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
    message_parts = [
        f"[info][title]ğŸ¤– æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ - {current_date}[/title]",
        f"ğŸ“… é…ä¿¡æ™‚åˆ»: {current_time}",
        f"ğŸ“Š è¨˜äº‹æ•°: {len(articles)}ä»¶",
        f"ğŸ“¡ æƒ…å ±æº: {len(sources)}ã‚µã‚¤ãƒˆ",
        "",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        ""
    ]
    
    # è¨˜äº‹ãƒªã‚¹ãƒˆéƒ¨åˆ†
    for i, article in enumerate(articles, 1):
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•åˆ¶é™
        title = article['title']
        if len(title) > 75:
            title = title[:72] + "..."
        
        source = article.get('source', 'ä¸æ˜')
        
        message_parts.extend([
            f"ğŸ“° ã€è¨˜äº‹ {i}ã€‘({source})",
            f"ğŸ’¡ {title}",
            f"ğŸ”— {article['link']}",
            ""
        ])
    
    # æƒ…å ±æºã‚µãƒãƒªãƒ¼ï¼ˆä¸Šä½5ã¤ã¾ã§è¡¨ç¤ºï¼‰
    if len(sources) > 1:
        message_parts.extend([
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "ğŸ“ˆ ä¸»è¦æƒ…å ±æº:",
            ""
        ])
        
        sorted_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)[:5]
        for source, count in sorted_sources:
            message_parts.append(f"ã€€â€¢ {source}: {count}ä»¶")
        
        message_parts.append("")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼éƒ¨åˆ†
    message_parts.extend([
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        "âœ¨ æœ€æ–°ã®AIæƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã—ãŸï¼",
        "ğŸ“± æ°—ã«ãªã‚‹è¨˜äº‹ãŒã‚ã‚Œã°ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã”è¦§ãã ã•ã„ã€‚",
        "[/info]"
    ])
    
    return "\n".join(message_parts)


def build_no_news_message() -> str:
    """
    è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã€‚
    """
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M")
    
    return (
        f"[info][title]ğŸ¤– æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ - {current_date}[/title]\n"
        f"ğŸ“… é…ä¿¡æ™‚åˆ»: {current_time}\n"
        f"ğŸ“Š è¨˜äº‹æ•°: 0ä»¶\n\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        f"ğŸ” ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æœ¬æ—¥ã¯æ–°ã—ã„AIé–¢é€£è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        f"ğŸ“° æ˜æ—¥ã¾ãŸæœ€æ–°æƒ…å ±ã‚’ãŠå±Šã‘ã„ãŸã—ã¾ã™ï¼\n\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        f"[/info]"
    )


def build_error_message() -> str:
    """
    ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã€‚
    """
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M")
    
    return (
        f"[info][title]âš ï¸ ã‚·ã‚¹ãƒ†ãƒ é€šçŸ¥ - {current_date}[/title]\n"
        f"ğŸ“… é€šçŸ¥æ™‚åˆ»: {current_time}\n\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        f"ğŸš¨ AIãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n"
        f"ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«ã”ç¢ºèªãã ã•ã„ã€‚\n"
        f"ğŸ• æ¬¡å›ã®é…ä¿¡ã‚’ãŠå¾…ã¡ãã ã•ã„ã€‚\n\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        f"[/info]"
    )


def main():
    try:
        news_list = fetch_ai_news()
        
        if not news_list:
            logging.warning("å–å¾—ã§ããŸAIè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            no_news_msg = build_no_news_message()
            post_to_chatwork(no_news_msg)
            return

        # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦æŠ•ç¨¿
        unified_msg = build_news_message(news_list)
        post_to_chatwork(unified_msg)
        
        logging.info(f"AIãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(news_list)}ä»¶ã‚’ä¸€æ‹¬æŠ•ç¨¿ã—ã¾ã—ãŸã€‚")
        
    except Exception as e:
        logging.exception("ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®é€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        error_msg = build_error_message()
        try:
            post_to_chatwork(error_msg)
        except:
            logging.error("ã‚¨ãƒ©ãƒ¼é€šçŸ¥ã®æŠ•ç¨¿ã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
